---
title: "1361 Final"
author: "Max Bauer"
date: "2024-04-14"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(stats)
```

Reading in data

```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")

## Viewing both table summaries without the album or track name, and id number columns

summary(train[,-(1:3)])

summary(test[,-(1:3)])
```

```{r}
train <- train[, -(1:3)]
colSums(is.na(train))
```

No NA values in train data

Checking distribution of popularity

```{r}
summary(train$popularity)
hist(train$popularity, breaks = 20, main = "Histogram of Popularity")
```

The data is positively skewed (right-skewed), as the mean is higher than the median. The majority of the data seems to be concentrated towards the lower end, with a long tail towards the higher values.

```{r}
library(moments)
# Check the skewness of the target variable
print("Skewness of the 'popularity' variable before transformation:")
print(skewness(train$popularity))

# Apply square root transformation to the 'popularity' variable
train$popularity_sqrt <- sqrt(train$popularity)

# Check the skewness after transformation
print("Skewness of the 'popularity' variable after square root transformation:")
print(skewness(train$popularity_sqrt))
```

```{r}
summary(train$popularity_sqrt)
hist(train$popularity_sqrt, breaks = 20, main = "Histogram of Popularity")
```

Skewness improved to .412 and there is a smaller range between the minimum and maximum values and the lower difference between the third quartile and the maximum. In both distributions, the mean is higher than the median, indicating positive skewness. However, the difference between the mean and median is much larger in the pre-transformed distribution, suggesting that there might be some extreme values pulling the mean upwards.

Correlation matrix

```{r}
correlation_matrix <- cor(train[, sapply(train, is.numeric)])
print(correlation_matrix)
```
Making categorical variable for genre. Jazz, Rock, and pop coded into 0, 1, and 2 respectively.

```{r}
train$track_genre <- factor(train$track_genre, levels = c("jazz", "rock", "pop"))

# Convert factor levels to numeric values
train$track_genre <- as.numeric(train$track_genre) - 1

# Check the unique values
unique(train$track_genre)

```


Lets look at each metric with popularity

```{r}
with(train, plot(duration_ms, popularity))
with(train, plot(explicit, popularity))
with(train, plot(danceability, popularity))
with(train, plot(energy, popularity))
with(train, plot(key, popularity))
with(train, plot(loudness, popularity))
with(train, plot(mode, popularity))
with(train, plot(speechiness, popularity))
with(train, plot(acousticness, popularity))
with(train, plot(instrumentalness, popularity))
with(train, plot(liveness, popularity))
with(train, plot(valence, popularity))
with(train, plot(tempo, popularity))
with(train, plot(time_signature, popularity))
with(train, plot(track_genre, popularity))
```


```{r}
with(train, plot(duration_ms, popularity_sqrt))
with(train, plot(explicit, popularity_sqrt))
with(train, plot(danceability, popularity_sqrt))
with(train, plot(energy, popularity_sqrt))
with(train, plot(key, popularity_sqrt))
with(train, plot(loudness, popularity_sqrt))
with(train, plot(mode, popularity_sqrt))
with(train, plot(speechiness, popularity_sqrt))
with(train, plot(acousticness, popularity_sqrt))
with(train, plot(instrumentalness, popularity_sqrt))
with(train, plot(liveness, popularity_sqrt))
with(train, plot(valence, popularity_sqrt))
with(train, plot(tempo, popularity_sqrt))
with(train, plot(time_signature, popularity_sqrt))
with(train, plot(track_genre, popularity_sqrt))
```

Its not a huge difference but it looks a little better so we will  continue to test it along with the untransformed.

```{r}
library(stats)
library(boot)

#validation set split
set.seed(4)
trainindex <- sample(1:nrow(train), nrow(train)*.7)
X_train <- train[trainindex, c(-1,-17)]
Y_train <- train[trainindex, "popularity"]
Y_train_sqrt <- train[trainindex, "popularity_sqrt"]

X_test <- train[-trainindex, c(-1, -17)]
Y_test <- train[-trainindex, "popularity"]
Y_test_sqrt <- train[-trainindex, "popularity_sqrt"]
```

```{r}
#linear model with all predictors
set.seed(4)
lmfull <- lm(Y_train ~ ., data = X_train)
summary(lmfull)
pred_lmfull <- predict(lmfull, newdata = X_test)
mse_lmfull = mean((Y_test - pred_lmfull)^2)
print(mse_lmfull)

#linear model with all predictors sqrt
set.seed(4)
lmfull_sqrt <- lm(Y_train_sqrt ~ ., data = X_train)
summary(lmfull_sqrt)
pred_lmfull_sqrt <- predict(lmfull_sqrt, newdata = X_test)
mse_lmfull_sqrt = mean((Y_test_sqrt - pred_lmfull_sqrt)^2)
print(mse_lmfull_sqrt)
```

A MSE of the popularity transformed improved from 959 to 12, but the MSE values on the transformed scale are not directly comparable to MSE values on the original scale. The model did improve very sligthly on the r^2, adj r^2, and F-stat so we will use the square root popularity from here on out.

```{r}
train <- train[, -1]

reducedlm_sqrt <- lm(Y_train_sqrt ~ duration_ms + danceability + mode + speechiness + acousticness + valence + tempo + track_genre, data = X_train)

pred_lmreduced_sqrt <- predict(reducedlm_sqrt, newdata = X_test)
mse_lmreduced_sqrt = mean((Y_test_sqrt - pred_lmreduced_sqrt)^2)
print(mse_lmreduced_sqrt)
```

Ridge

```{r}
set.seed(4)
library(glmnet)

x = model.matrix(popularity_sqrt~., train)[, -1]
y = train$popularity_sqrt

grid <- 10^seq(10,-2, length = 100)
ridge <- glmnet(x, y, alpha = 0, lambda = grid)

#use cross validation to choose best lambda value
cvridge <- cv.glmnet(x, y, alpha = 0, lambda = grid)
best <- cvridge$lambda.min
best

mseridge <- cvridge$cvm[which(cvridge$lambda == best)]
mseridge

#obtaining the coefficients of the model chosen by CV
model <- predict(ridge, s = best, type = "coefficients")
model
```

Lasso

```{r}
lasso <- glmnet(x, y, alpha = 1, lambda = grid)

#finding the best lambda value by CV
set.seed(4)
cvlasso <- cv.glmnet(x, y, alpha = 1)
best <- cvlasso$lambda.min

#obtaining the CV error of the lasso model chosen by CV
pred <-  predict(lasso, newdata = X_test, s = best, type = "response", newx = as.matrix(X_test))
mselasso <- mean((Y_test_sqrt - pred)^2)
mselasso

predict(lasso, s = best, type = "coefficients")
```

Tree

```{r}
library(tree)

tree <- tree(Y_train_sqrt ~., data = X_train)
summary(tree)
```

```{r}
plot(tree)
text(tree, pretty = 0, cex = 0.5)
```

```{r}
pred <- predict(tree, newdata = X_test)
msetree <- mean((Y_test_sqrt - pred)^2)
msetree
```

```{r}
cvtree <- cv.tree(tree)
plot(cvtree$size, cvtree$dev, type = 'b')
```

```{r}
#choose size 15 as the pruned tree
prune = prune.tree(tree, best = 14)
plot(prune)
text(prune, pretty = 0, cex = 0.5)
```

```{r}
pred <-  predict(prune, newdata = X_test)
mseprune <- mean((Y_test_sqrt - pred)^2)
mseprune
```

Bagging, Random Forest

```{r}
set.seed(4)
library(randomForest)

#obtain bagging model
bag <- randomForest(Y_train_sqrt ~., data = X_train, mtry = 15, importance = TRUE)
bag

pred <-  predict(bag, newdata = X_test)
msebag <- mean((Y_test_sqrt - pred)^2)
msebag
```

```{r}
# View variable importance
variable_importance <- importance(bag)

# Print the variable importance
print(variable_importance)

# Visualize variable importance
varImpPlot(bag)
```


```{r}
#random forest tuning over mtry using validation set approach
MSEs <- rep(NA, 15)
for(i in 1:15){
        rf <- randomForest(Y_train_sqrt ~., data = X_train, mtry = i)
        pred <-  predict(rf, newdata = X-train)
        MSEs[i-2] <- mean((Y_test_sqrt - pred)^2)
}

plot(1:15, MSEs, type = "b")
```

```{r}
#obtain the validation set MSE for the best rf model
bestrf <- randomForest(Y_train_sqrt~., data = X_train, mtry = (which.min(MSEs) + 2))
pred <-  predict(bestrf, newdata = X_test)
mserf <- mean((Y_test_sqrt - pred)^2)

mserf
varImpPlot(bestrf)
```

```{r}
importance(bestrf)
```

Boosting

```{r}
library(gbm)

X_train$explicit <- as.numeric(X_train$explicit)
X_test$explicit <- as.numeric(X_test$explicit)

set.seed(4)
grid <- c(10^seq(-5, -1), .2, .3)
ntree <- c(50, 100, 250, 500, 1000, 1500, 2000, 3000)
MSEs <- matrix(nrow = length(ntree), ncol = length(grid))
for(i in 1:length(grid)){
        for(j in 1:length(ntree)){
                boost <- gbm(Y_train_sqrt ~., data = X_train, n.trees = ntree[j], distribution = "gaussian", interaction.depth = 4, shrinkage = grid[i])
                pred <-  predict(boost, newdata = X_test)
                MSEs[j,i] <- mean((Y_test_sqrt - pred)^2)
        }
}

MSEs
```

```{r}
which <- which.min(MSEs)

#obtaining the validation set error of the best boosting model
boost <- gbm(Y_train_sqrt ~., data = X_train, n.trees = ntree[(which %% 7)], distribution = "gaussian", interaction.depth = 4, shrinkage = grid[round(which/8, 0) + 1])
pred <-  predict(boost, newdata = X_test)

mseboost <- mean((Y_test_sqrt - pred)^2 )
mseboost
```
```{r}
summary(boost)
```

```{r}
data.frame(mse_lmfull_sqrt, mse_lmreduced_sqrt, mselasso, mseafterlasso, mseridge, msetree, mseprune, mserf, msebag, mseboost)
```

Predictions
 
```{r}
test_set_pred <- test[, -(1:3)]
test$popularity_pred <- predict(rf, newdata = test_set_pred)
test_results <- test[, c(1,19)]

write.csv(test_results, file = "testing_predictions_Bauer_Max_mbb64.csv", row.names = FALSE)
```

